{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import  transformers\n",
    "import transformers.models\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "config = transformers.models.bert.BertConfig()\n",
    "model = transformers.models.bert.BertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# model.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "from transformers import AutoModelForMaskedLM\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Rectangle\n",
    "from glob import glob\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold\n",
      "(1.0, 1.0, 0.0, 0.8136860244632429, 0.7896173224568138)\n"
     ]
    }
   ],
   "source": [
    "langset= \"jaen_eval_past100\"\n",
    "paths = glob(f\"../data/awesome_eval_data/{langset}.*\")\n",
    "# print(paths)\n",
    "df = pd.DataFrame({path.split(f'/')[-1].replace(f\"{langset}.\", \"\"): open(path, \"r\", encoding='utf-8').read().split('\\n')[:-1] for path in paths})\n",
    "# datum = df.iloc[130] # [\"nofinetune.out\"]\n",
    "def prepare_raw_datum(raw_gold, raw_hypothesis, raw_srt_tgt):\n",
    "    src, tgt = [d.strip().split(\" \") for d in raw_srt_tgt.split(\" ||| \")]\n",
    "    m = len(src)\n",
    "    l = len(tgt)\n",
    "    gold_mat = np.zeros((m, l)) # assume gold is 1 indexed, so subtract by 1.\n",
    "    sub = 1\n",
    "    if \"jaen\" in langset and \"awesome_test_examples\" not in paths[0]:\n",
    "        sub = 0\n",
    "    possible_gold_mat = [tuple(int(i)-sub for i in out.replace('-', 'p').split('p')) for out in raw_gold.strip().split(' ')]\n",
    "    sure_gold_mat = [tuple(int(i)-sub for i in out.split('-')) for out in raw_gold.strip().split(' ') if \"-\" in out]\n",
    "    gold_mat[tuple(zip(*possible_gold_mat))] = 0.5\n",
    "    gold_mat[tuple(zip(*sure_gold_mat))] = 1\n",
    "\n",
    "    out_mat = np.zeros((m, l))\n",
    "    sure_hypo_mat = [tuple(int(i)-sub for i in out.split('-')) for out in raw_hypothesis.split(' ') if \"-\" in out]\n",
    "    out_mat[tuple(zip(*sure_hypo_mat))] = 1\n",
    "\n",
    "    return gold_mat, out_mat, src, tgt\n",
    "def get_prec_recall_aer(sure, possible, hypothesis, srcs, tgts, return_impact=False):\n",
    "    total_sure = 0\n",
    "    total_guessed = 0\n",
    "    # recall is the ones you can get right out of the ones there are to get right.\n",
    "    # this is the A intersect sure / sure\n",
    "    total_recall_numerator = 0\n",
    "\n",
    "    # precision is the ones you get right out of the ones you guessed.\n",
    "    # this is A interect possible / A\n",
    "    total_precision_numerator = 0\n",
    "    coverages = []\n",
    "\n",
    "    for s, p, h in zip(sure, possible, hypothesis):\n",
    "        total_sure += len(s)\n",
    "        total_guessed += len(h)\n",
    "        total_recall_numerator += len(set(s).intersection(h))\n",
    "        total_precision_numerator += len(set(h).intersection(p))\n",
    "    precision = total_precision_numerator / total_guessed\n",
    "    recall = total_recall_numerator / total_sure\n",
    "    aer = 1 - (total_recall_numerator + total_precision_numerator) / (total_sure + total_guessed)\n",
    "\n",
    "    num_sure = 0\n",
    "    guesses_made = 0\n",
    "    guesses_made_in_possible = 0\n",
    "    guesses_made_in_sure = 0\n",
    "\n",
    "    num_words_total = 0\n",
    "    num_words_covered = 0\n",
    "    coverages = []\n",
    "    for gold_sure_word_alignment, gold_possible_word_alignment, word_level_alignment, src, tgt in zip(sure, possible, hypothesis, srcs, tgts):\n",
    "    \n",
    "        num_sure += len(gold_sure_word_alignment)\n",
    "        guesses_made += len(word_level_alignment)\n",
    "        guesses_made_in_sure += len(set(word_level_alignment).intersection(gold_sure_word_alignment))\n",
    "        guesses_made_in_possible += len(set(word_level_alignment).intersection(gold_possible_word_alignment))\n",
    "        num_words_total += len(src) + len(tgt) # this doesn't account for the fact we only take the first 510 tokens to align.\n",
    "        num_words_covered += len(set(s for s,t in word_level_alignment)) + len(set(t for s,t in word_level_alignment))\n",
    "        coverages.append((len(set(s for s,t in word_level_alignment)) + len(set(t for s,t in word_level_alignment))) / (len(src) + len(tgt)))\n",
    "    precision2 = guesses_made_in_possible / guesses_made\n",
    "    recall2 = guesses_made_in_sure / num_sure\n",
    "    aer2 = 1 - ((guesses_made_in_possible + guesses_made_in_sure) / (guesses_made + num_sure))\n",
    "    coverage = sum(coverages) / len(coverages)\n",
    "    coverage2 = num_words_covered / num_words_total\n",
    "    assert (precision == precision2)\n",
    "    assert (recall == recall2)\n",
    "    assert (aer == aer2)\n",
    "\n",
    "    if return_impact:\n",
    "        impact = (total_sure + total_guessed) - (total_recall_numerator + total_precision_numerator)\n",
    "        return precision2, recall2, aer2, coverage, coverage2, impact\n",
    "    else:\n",
    "        return precision2, recall2, aer2, coverage, coverage2\n",
    "\n",
    "# display_alignment(*prepare_raw_datum(datum[\"gold\"], datum['nofinetune.out'], datum['src-tgt']))\n",
    "\n",
    "for out_format in [\"gold\"]: # , \"fast-grow-diagonal\"\n",
    "    sure = []\n",
    "    possible = []\n",
    "    hypothesis = []\n",
    "    srcs = []\n",
    "    tgts = []\n",
    "    for i in range(len(df)):\n",
    "        datum = df.iloc[i]\n",
    "        gold_mat, out_mat, src, tgt = prepare_raw_datum(datum[\"gold\"], datum[\"gold\"], datum['src-tgt'])\n",
    "        \n",
    "        s = list(zip(*np.where(gold_mat==1)))\n",
    "        p = list(zip(*np.where(gold_mat>=0.5)))\n",
    "        h = list(zip(*np.where(out_mat==1)))\n",
    "        df.loc[i,out_format + \"precision\"], df.loc[i,out_format + \"recall\"], df.loc[i,out_format + \"aer\"], df.loc[i,out_format + \"coverage\"], df.loc[i,out_format + \"coverage2\"], df.loc[i, out_format + \"impact_on_aer\"] = get_prec_recall_aer([s], [p], [h], [src], [tgt], return_impact=True) # type: ignore\n",
    "        \n",
    "        sure.append(s)\n",
    "        possible.append(p)\n",
    "        hypothesis.append(h)\n",
    "        srcs.append(src)\n",
    "        tgts.append(tgt)\n",
    "    print(out_format)\n",
    "    print(get_prec_recall_aer(sure, possible, hypothesis, srcs, tgts))\n",
    "    # print(df.sort_values(out_format + 'impact_on_aer').index.tolist()[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=119547, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jaen_eval_past100: 0.7896173224568138"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "greek-v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
